{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMoYplq73qj57UfYZzmRjnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimsooyoung/rl_oc_python/blob/main/rl_oc_python%20/oc_lec3_actor_critic/PPO_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements"
      ],
      "metadata": {
        "id": "w_AxxYhErLww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YDYj1MUPc6M5",
        "outputId": "edd79c5b-63d5-46d8-8fbf-eb0a5e708d3e",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting swig\n",
            "  Downloading swig-4.2.1-py2.py3-none-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: swig\n",
            "Successfully installed swig-4.2.1\n",
            "Collecting renderlab\n",
            "  Downloading renderlab-0.1.20230421184216-py3-none-any.whl (4.0 kB)\n",
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.10/dist-packages (from renderlab) (1.0.3)\n",
            "Collecting gymnasium (from renderlab)\n",
            "  Downloading gymnasium-0.29.1-py3-none-any.whl (953 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m953.9/953.9 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->renderlab) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->renderlab) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium->renderlab) (4.11.0)\n",
            "Collecting farama-notifications>=0.0.1 (from gymnasium->renderlab)\n",
            "  Downloading Farama_Notifications-0.0.4-py3-none-any.whl (2.5 kB)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->renderlab) (4.4.2)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.10/dist-packages (from moviepy->renderlab) (4.66.4)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from moviepy->renderlab) (2.31.0)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->renderlab) (0.1.10)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.10/dist-packages (from moviepy->renderlab) (2.31.6)\n",
            "Requirement already satisfied: imageio-ffmpeg>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from moviepy->renderlab) (0.4.9)\n",
            "Requirement already satisfied: pillow<10.1.0,>=8.3.2 in /usr/local/lib/python3.10/dist-packages (from imageio<3.0,>=2.5->moviepy->renderlab) (9.4.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from imageio-ffmpeg>=0.2.0->moviepy->renderlab) (67.7.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0,>=2.8.1->moviepy->renderlab) (2024.2.2)\n",
            "Installing collected packages: farama-notifications, gymnasium, renderlab\n",
            "Successfully installed farama-notifications-0.0.4 gymnasium-0.29.1 renderlab-0.1.20230421184216\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.10/dist-packages (0.29.1)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (1.25.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.2.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.11.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Collecting box2d-py==2.3.5 (from gymnasium[box2d])\n",
            "  Downloading box2d-py-2.3.5.tar.gz (374 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m374.4/374.4 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (2.5.2)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.10/dist-packages (from gymnasium[box2d]) (4.2.1)\n",
            "Building wheels for collected packages: box2d-py\n",
            "  Building wheel for box2d-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for box2d-py: filename=box2d_py-2.3.5-cp310-cp310-linux_x86_64.whl size=2376102 sha256=e3c1dfbedd51fc0687ee198edf4a1ce8961f56fb1d16152365782e69145d0e3f\n",
            "  Stored in directory: /root/.cache/pip/wheels/db/8f/6a/eaaadf056fba10a98d986f6dce954e6201ba3126926fc5ad9e\n",
            "Successfully built box2d-py\n",
            "Installing collected packages: box2d-py\n",
            "Successfully installed box2d-py-2.3.5\n"
          ]
        }
      ],
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install swig\n",
        "!pip install renderlab\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the Necessary Packages"
      ],
      "metadata": {
        "id": "iFwAvznPrREV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "egarGY3ec9XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Render Test"
      ],
      "metadata": {
        "id": "QRYZ41zrrTxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import renderlab as rl\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "observation, info = env.reset()\n",
        "score = 0\n",
        "\n",
        "while True:\n",
        "  action = env.action_space.sample()\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "  score += reward\n",
        "\n",
        "  if terminated:\n",
        "    print(\"Score : \", score)\n",
        "    break\n",
        "\n",
        "env.play()"
      ],
      "metadata": {
        "id": "aSN_a4edrUd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Hyper Params"
      ],
      "metadata": {
        "id": "mmV01OPErXUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 0.0002\n",
        "gamma         = 0.98\n",
        "n_rollout     = 10"
      ],
      "metadata": {
        "id": "kgCL8yzjdCTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Actor-Critic Class\n",
        "\n",
        "- input: 4 length tensor\n",
        "- Actor layer structure: (4 * 256) = ReLU > (256 * 2) = softmax>\n",
        "- Critic layer structure: (4 * 256) = ReLU > (256 * 1)\n",
        "\n",
        "- `put_data` method: append episodes ($S, A, R, S', \\Pi(a, s), Done$) into class variable.\n",
        "- `make_batch` method : return torch type transitions and clear buffer\n",
        "- `train_net` method : optimize network with proximal policy gradient loss + Q-learning loss\n",
        "\n",
        "$for \\ 2-3 \\ epoch$\n",
        "\n",
        "$\\quad \\quad R = \\frac{P_{\\theta_{new}}(a, s)}{P_{\\theta_{old}}(a, s)} $\n",
        "\n",
        "$\\quad \\quad S_1 = RA, (A = advantage)$\n",
        "\n",
        "$\\quad \\quad S_2 = clamp(RA, 1-ϵ, 1+ϵ)$\n",
        "\n",
        "$\\quad \\quad loss = -min(S_1, S_2) + E[(R + rV(s) - V(s'))^2]$\n",
        "\n",
        "$end$\n"
      ],
      "metadata": {
        "id": "oTf7pPbmrwwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PPO, self).__init__()\n",
        "    self.data = []\n",
        "\n",
        "    self.fc1 = nn.Linear(4, 256)\n",
        "    self.fc_pi = nn.Linear(256, 2)\n",
        "    self.fc_v = nn.Linear(256, 1)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "  def pi(self, state, softmax_dim=0):\n",
        "    x1 = F.relu(self.fc1(state))\n",
        "    x2 = F.softmax(self.fc_pi(x1), dim=softmax_dim)\n",
        "    return x2\n",
        "\n",
        "  def v(self, state):\n",
        "    x1 = F.relu(self.fc1(state))\n",
        "    x2 = self.fc_v(x1)\n",
        "    return x2\n",
        "\n",
        "  def put_data(self, transition):\n",
        "    self.data.append(transition)\n",
        "\n",
        "  def make_batch(self):\n",
        "    s_list, a_list, r_list, sp_list, pa_list, done_list = [],[],[],[],[],[]\n",
        "\n",
        "    for transition in self.data:\n",
        "      s, a, r, sp, pa, done = transition\n",
        "      s_list.append(s)\n",
        "      a_list.append([a])\n",
        "      r_list.append([r/100.0])\n",
        "      sp_list.append(sp)\n",
        "      pa_list.append([pa])\n",
        "      done = 0.0 if done else 1.0\n",
        "      done_list.append([done])\n",
        "\n",
        "    s_batch, a_batch, r_batch, sp_batch, pa_batch, done_batch  = \\\n",
        "      torch.tensor(s_list, dtype=torch.float), torch.tensor(a_list), \\\n",
        "      torch.tensor(r_list,  dtype=torch.float), torch.tensor(sp_list, dtype=torch.float), \\\n",
        "      torch.tensor(pa_list), torch.tensor(done_list,  dtype=torch.float)\n",
        "\n",
        "    self.data = []\n",
        "\n",
        "    return s_batch, a_batch, r_batch, sp_batch, pa_batch, done_batch\n",
        "\n",
        "  def train_net(self):\n",
        "    s, a, r, sp, prob_a, done = self.make_batch()\n",
        "\n",
        "    for _ in range(K_epoch):\n",
        "      td_target = r + gamma * self.v(sp) * done\n",
        "      delta = td_target - self.v(s)\n",
        "      delta = delta.detach().numpy()\n",
        "\n",
        "      advantage = 0.0\n",
        "      advantage_list = []\n",
        "      for i, _delta in enumerate(delta[::-1]):\n",
        "        advantage = gamma * lmbda * advantage * done[i] + _delta[0]\n",
        "        advantage_list.append([advantage])\n",
        "      advantage_list.reverse()\n",
        "      advantage = torch.tensor(advantage_list, dtype=torch.float)\n",
        "\n",
        "      pi_old = prob_a\n",
        "      pi_new = self.pi(s).gather(1, a)\n",
        "      ratio = torch.exp(torch.log(pi_new) - torch.log(pi_old))\n",
        "\n",
        "      sur1 = ratio * advantage\n",
        "      sur2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
        "      loss = -torch.min(sur1, sur2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      self.optimizer.step()"
      ],
      "metadata": {
        "id": "IgAlv46qdDIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main loop\n",
        "\n",
        "- create environments, PPO Model, print_interval, and reset score value\n",
        "- for each train epochs\n",
        "  - reset environment\n",
        "  - for each episodes loop\n",
        "    - rollout $T_{horizon}$ times\n",
        "      - obtain policy action probability and actual action value\n",
        "      - step environment\n",
        "      - gather transitions $(S, A, R, S', \\Pi(a, s), Done)$\n",
        "      - put transition into dataset\n",
        "      - update state, update score\n",
        "    - train network\n",
        "  - print progress\n",
        "- close env"
      ],
      "metadata": {
        "id": "3SOAzfY5uIBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "ppo = PPO()\n",
        "\n",
        "score = 0.0\n",
        "print_interval = 500\n",
        "\n",
        "for epi in range(10000):\n",
        "  s, _ = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    for _ in range(100):\n",
        "      prob = ppo.pi( torch.from_numpy(s).float() )\n",
        "      m = Categorical(prob)\n",
        "      a = m.sample().item()\n",
        "      sp, r, done, truncated, info = env.step(a)\n",
        "      ppo.put_data( (s, a, r/100.0, sp, prob[a].item(), done) )\n",
        "\n",
        "      s = sp\n",
        "      score += r\n",
        "\n",
        "      if done or truncated:\n",
        "        break\n",
        "\n",
        "    ppo.train_net()\n",
        "\n",
        "  if (epi % print_interval == 0) and (epi != 0):\n",
        "    print(f\"epi: {epi} / avg_score: {score / print_interval}\")\n",
        "    score = 0.0\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f_CHSKJd3l1",
        "outputId": "63de38d3-2cf6-417a-f8ff-2d095ab984b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epi: 100 / avg_score: 26.76\n",
            "epi: 200 / avg_score: 37.2\n",
            "epi: 300 / avg_score: 51.88\n",
            "epi: 400 / avg_score: 95.49\n",
            "epi: 500 / avg_score: 170.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play with the result"
      ],
      "metadata": {
        "id": "zfxDDQjuvAUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "s, info = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  prob = ac_nn.pi( torch.from_numpy(s).float() )\n",
        "  m = Categorical(prob)\n",
        "  a = m.sample().item()\n",
        "  sp, r, done, truncated, info = env.step(a)\n",
        "\n",
        "  s = sp\n",
        "  score += r\n",
        "\n",
        "  if done or truncated:\n",
        "    print(\"Score : \", score)\n",
        "    break\n",
        "\n",
        "env.play()"
      ],
      "metadata": {
        "id": "kMDsiwq2ipPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
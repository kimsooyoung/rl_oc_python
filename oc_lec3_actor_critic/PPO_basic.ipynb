{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMoYplq73qj57UfYZzmRjnJ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kimsooyoung/rl_oc_python/blob/main/oc_lec3_actor_critic/PPO_basic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install Requirements"
      ],
      "metadata": {
        "id": "w_AxxYhErLww"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YDYj1MUPc6M5",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
        "!pip install swig\n",
        "!pip install renderlab\n",
        "!pip install gymnasium\n",
        "!pip install gymnasium[box2d]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Import the Necessary Packages"
      ],
      "metadata": {
        "id": "iFwAvznPrREV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import collections\n",
        "import random\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.distributions import Categorical"
      ],
      "metadata": {
        "id": "egarGY3ec9XK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Render Test"
      ],
      "metadata": {
        "id": "QRYZ41zrrTxb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import renderlab as rl\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "\n",
        "observation, info = env.reset()\n",
        "score = 0\n",
        "\n",
        "while True:\n",
        "  action = env.action_space.sample()\n",
        "  observation, reward, terminated, truncated, info = env.step(action)\n",
        "  score += reward\n",
        "\n",
        "  if terminated:\n",
        "    print(\"Score : \", score)\n",
        "    break\n",
        "\n",
        "env.play()"
      ],
      "metadata": {
        "id": "aSN_a4edrUd4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initialize Hyper Params"
      ],
      "metadata": {
        "id": "mmV01OPErXUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Hyperparameters\n",
        "learning_rate = 0.0002\n",
        "gamma         = 0.98\n",
        "n_rollout     = 10"
      ],
      "metadata": {
        "id": "kgCL8yzjdCTh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Define Actor-Critic Class\n",
        "\n",
        "- input: 4 length tensor\n",
        "- Actor layer structure: (4 * 256) = ReLU > (256 * 2) = softmax>\n",
        "- Critic layer structure: (4 * 256) = ReLU > (256 * 1)\n",
        "\n",
        "- `put_data` method: append episodes ($S, A, R, S', \\Pi(a, s), Done$) into class variable.\n",
        "- `make_batch` method : return torch type transitions and clear buffer\n",
        "- `train_net` method : optimize network with proximal policy gradient loss + Q-learning loss\n",
        "\n",
        "$for \\ 2-3 \\ epoch$\n",
        "\n",
        "$\\quad \\quad R = \\frac{P_{\\theta_{new}}(a, s)}{P_{\\theta_{old}}(a, s)} $\n",
        "\n",
        "$\\quad \\quad S_1 = RA, (A = advantage)$\n",
        "\n",
        "$\\quad \\quad S_2 = clamp(RA, 1-ϵ, 1+ϵ)$\n",
        "\n",
        "$\\quad \\quad loss = -min(S_1, S_2) + E[(R + rV(s) - V(s'))^2]$\n",
        "\n",
        "$end$\n"
      ],
      "metadata": {
        "id": "oTf7pPbmrwwc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PPO(nn.Module):\n",
        "\n",
        "  def __init__(self):\n",
        "    super(PPO, self).__init__()\n",
        "    self.data = []\n",
        "\n",
        "    self.fc1 = nn.Linear(4, 256)\n",
        "    self.fc_pi = nn.Linear(256, 2)\n",
        "    self.fc_v = nn.Linear(256, 1)\n",
        "\n",
        "    self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
        "\n",
        "  def pi(self, state, softmax_dim=0):\n",
        "    x1 = F.relu(self.fc1(state))\n",
        "    x2 = F.softmax(self.fc_pi(x1), dim=softmax_dim)\n",
        "    return x2\n",
        "\n",
        "  def v(self, state):\n",
        "    x1 = F.relu(self.fc1(state))\n",
        "    x2 = self.fc_v(x1)\n",
        "    return x2\n",
        "\n",
        "  def put_data(self, transition):\n",
        "    self.data.append(transition)\n",
        "\n",
        "  def make_batch(self):\n",
        "    s_list, a_list, r_list, sp_list, pa_list, done_list = [],[],[],[],[],[]\n",
        "\n",
        "    for transition in self.data:\n",
        "      s, a, r, sp, pa, done = transition\n",
        "      s_list.append(s)\n",
        "      a_list.append([a])\n",
        "      r_list.append([r/100.0])\n",
        "      sp_list.append(sp)\n",
        "      pa_list.append([pa])\n",
        "      done = 0.0 if done else 1.0\n",
        "      done_list.append([done])\n",
        "\n",
        "    s_batch, a_batch, r_batch, sp_batch, pa_batch, done_batch  = \\\n",
        "      torch.tensor(s_list, dtype=torch.float), torch.tensor(a_list), \\\n",
        "      torch.tensor(r_list,  dtype=torch.float), torch.tensor(sp_list, dtype=torch.float), \\\n",
        "      torch.tensor(pa_list), torch.tensor(done_list,  dtype=torch.float)\n",
        "\n",
        "    self.data = []\n",
        "\n",
        "    return s_batch, a_batch, r_batch, sp_batch, pa_batch, done_batch\n",
        "\n",
        "  def train_net(self):\n",
        "    s, a, r, sp, prob_a, done = self.make_batch()\n",
        "\n",
        "    for _ in range(K_epoch):\n",
        "      td_target = r + gamma * self.v(sp) * done\n",
        "      delta = td_target - self.v(s)\n",
        "      delta = delta.detach().numpy()\n",
        "\n",
        "      advantage = 0.0\n",
        "      advantage_list = []\n",
        "      for i, _delta in enumerate(delta[::-1]):\n",
        "        advantage = gamma * lmbda * advantage * done[i] + _delta[0]\n",
        "        advantage_list.append([advantage])\n",
        "      advantage_list.reverse()\n",
        "      advantage = torch.tensor(advantage_list, dtype=torch.float)\n",
        "\n",
        "      pi_old = prob_a\n",
        "      pi_new = self.pi(s).gather(1, a)\n",
        "      ratio = torch.exp(torch.log(pi_new) - torch.log(pi_old))\n",
        "\n",
        "      sur1 = ratio * advantage\n",
        "      sur2 = torch.clamp(ratio, 1-eps_clip, 1+eps_clip) * advantage\n",
        "      loss = -torch.min(sur1, sur2) + F.smooth_l1_loss(self.v(s) , td_target.detach())\n",
        "\n",
        "      self.optimizer.zero_grad()\n",
        "      loss.mean().backward()\n",
        "      self.optimizer.step()"
      ],
      "metadata": {
        "id": "IgAlv46qdDIo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Main loop\n",
        "\n",
        "- create environments, PPO Model, print_interval, and reset score value\n",
        "- for each train epochs\n",
        "  - reset environment\n",
        "  - for each episodes loop\n",
        "    - rollout $T_{horizon}$ times\n",
        "      - obtain policy action probability and actual action value\n",
        "      - step environment\n",
        "      - gather transitions $(S, A, R, S', \\Pi(a, s), Done)$\n",
        "      - put transition into dataset\n",
        "      - update state, update score\n",
        "    - train network\n",
        "  - print progress\n",
        "- close env"
      ],
      "metadata": {
        "id": "3SOAzfY5uIBn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\")\n",
        "ppo = PPO()\n",
        "\n",
        "score = 0.0\n",
        "print_interval = 500\n",
        "\n",
        "for epi in range(10000):\n",
        "  s, _ = env.reset()\n",
        "  done = False\n",
        "\n",
        "  while not done:\n",
        "    for _ in range(100):\n",
        "      prob = ppo.pi( torch.from_numpy(s).float() )\n",
        "      m = Categorical(prob)\n",
        "      a = m.sample().item()\n",
        "      sp, r, done, truncated, info = env.step(a)\n",
        "      ppo.put_data( (s, a, r/100.0, sp, prob[a].item(), done) )\n",
        "\n",
        "      s = sp\n",
        "      score += r\n",
        "\n",
        "      if done or truncated:\n",
        "        break\n",
        "\n",
        "    ppo.train_net()\n",
        "\n",
        "  if (epi % print_interval == 0) and (epi != 0):\n",
        "    print(f\"epi: {epi} / avg_score: {score / print_interval}\")\n",
        "    score = 0.0\n",
        "\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4f_CHSKJd3l1",
        "outputId": "63de38d3-2cf6-417a-f8ff-2d095ab984b3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "epi: 100 / avg_score: 26.76\n",
            "epi: 200 / avg_score: 37.2\n",
            "epi: 300 / avg_score: 51.88\n",
            "epi: 400 / avg_score: 95.49\n",
            "epi: 500 / avg_score: 170.55\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Play with the result"
      ],
      "metadata": {
        "id": "zfxDDQjuvAUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v1\", render_mode = \"rgb_array\")\n",
        "env = rl.RenderFrame(env, \"./output\")\n",
        "s, info = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "  prob = ac_nn.pi( torch.from_numpy(s).float() )\n",
        "  m = Categorical(prob)\n",
        "  a = m.sample().item()\n",
        "  sp, r, done, truncated, info = env.step(a)\n",
        "\n",
        "  s = sp\n",
        "  score += r\n",
        "\n",
        "  if done or truncated:\n",
        "    print(\"Score : \", score)\n",
        "    break\n",
        "\n",
        "env.play()"
      ],
      "metadata": {
        "id": "kMDsiwq2ipPN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}